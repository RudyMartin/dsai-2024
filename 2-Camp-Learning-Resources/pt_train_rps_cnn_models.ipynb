{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBOiqAQ7/om6QFU3TC/5q2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RudyMartin/dsai-2024/blob/main/pt_train_rps_cnn_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PYTORCH** Rock Paper Scissors with CNN Models\n",
        "Assumes running T4 GPU Backend on Google Colab"
      ],
      "metadata": {
        "id": "64GiCAXUC28y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import datetime\n",
        "\n",
        "# Start time\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# 1. Mount Google Drive to access the dataset\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# 2. Verify and list directories\n",
        "root_dir = '/content/gdrive/My Drive/'\n",
        "print(os.listdir(root_dir))\n",
        "\n",
        "rps_dir = os.path.join(root_dir, 'rps')\n",
        "if os.path.exists(rps_dir):\n",
        "    print(f\"'rps' directory contents: {os.listdir(rps_dir)}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Directory {rps_dir} does not exist.\")\n",
        "\n",
        "train_dir = os.path.join(rps_dir, 'train')\n",
        "test_dir = os.path.join(rps_dir, 'test')\n",
        "model_dir = os.path.join(root_dir, 'model')\n",
        "print(f\"rps directory contents: {os.listdir(rps_dir)}\")\n",
        "\n",
        "# 3. Data augmentation and normalization for training\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((160, 120)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(40),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=None, shear=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 4. Just normalization for testing\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((160, 120)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# 5. Load datasets\n",
        "train_dataset = ImageFolder(os.path.join(rps_dir, 'train'), transform=train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "test_dataset = ImageFolder(os.path.join(rps_dir, 'test'), transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# 6. Define the CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 20 * 15, 256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CNNModel().to(device)\n",
        "\n",
        "# 7. Compile the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# 8. Define a learning rate schedule (if needed)\n",
        "def scheduler(epoch):\n",
        "    if epoch < 10:\n",
        "        return 1.0\n",
        "    else:\n",
        "        return np.exp(-0.1)\n",
        "\n",
        "lr_scheduler = LambdaLR(optimizer, lr_lambda=scheduler)\n",
        "\n",
        "# 9. Train the model\n",
        "def train_model(epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "train_model(100)\n",
        "\n",
        "# 10. Save the trained model\n",
        "model_path = os.path.join(rps_dir, 'pt_model.pth')\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}.\")\n",
        "\n",
        "# 11. Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Test accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# 12. Accuracy per class\n",
        "Y_pred = []\n",
        "actual_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        Y_pred.extend(predicted.cpu().numpy())\n",
        "        actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(actual_labels, Y_pred))\n",
        "print('Classification Report')\n",
        "target_names = list(test_loader.dataset.classes)\n",
        "# Assuming actual_labels and Y_pred are your actual and predicted labels respectively\n",
        "# and target_names are the names of the classes\n",
        "print(classification_report(actual_labels, Y_pred, target_names=target_names, zero_division=0))\n",
        "\n",
        "\n",
        "# End time\n",
        "end_time = datetime.datetime.now()\n",
        "# Calculate duration\n",
        "duration = end_time - start_time\n",
        "# Print execution time\n",
        "print(f\"Execution time: {str(duration)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5vVRMdcEA82",
        "outputId": "92b93949-c917-40ff-b42e-1cf72daa7dd4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "['DSC02677.jpeg', 'Colab Notebooks', 'dscamp_2023', 'dscamp_2022', 'proportions.csv', 'proportions.gsheet', 'activeloop_2023', 'Untitled spreadsheet (1).gsheet', 'Untitled spreadsheet.gsheet', 'YouTube', 'Copy of My Presentation.gslides', 'Introduction .gslides', 'Copy of Introduction .gslides', 'dscamp', 'dscamp_2024_nano', 'ds_camp_2024_trans_hf', 'rps_test', 'rps', 'rps_pics', 'papers']\n",
            "'rps' directory contents: ['test', 'train', 'models', 'modelsbaseline_1.keras', 'sequential_acc_graph.png', 'sequential_loss_graph.png', 'df_metrics_20240610.csv', 'model.h5', 'model.keras', 'model.pth', 'pt_model.pth']\n",
            "rps directory contents: ['test', 'train', 'models', 'modelsbaseline_1.keras', 'sequential_acc_graph.png', 'sequential_loss_graph.png', 'df_metrics_20240610.csv', 'model.h5', 'model.keras', 'model.pth', 'pt_model.pth']\n",
            "Using device: cuda\n",
            "Model saved to /content/gdrive/My Drive/rps/pt_model.pth.\n",
            "Test accuracy: 50.00%\n",
            "Confusion Matrix\n",
            "[[6 0 0]\n",
            " [3 3 0]\n",
            " [5 1 0]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       paper       0.43      1.00      0.60         6\n",
            "        rock       0.75      0.50      0.60         6\n",
            "    scissors       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.50        18\n",
            "   macro avg       0.39      0.50      0.40        18\n",
            "weighted avg       0.39      0.50      0.40        18\n",
            "\n",
            "Execution time: 0:01:23.365385\n"
          ]
        }
      ]
    }
  ]
}