{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUufO188jsGRgd2ECHjIUp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RudyMartin/dsai-2024/blob/main/MVPS/Camp-Rock-Paper-Scissors/team_GG_AI/predict_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision: Re-size Image or Alter CNN Input Layer Shape"
      ],
      "metadata": {
        "id": "4Ds04j2PxMtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Background: (Need clear problem statement here)"
      ],
      "metadata": {
        "id": "4LfAO_Ak5wvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When deciding whether to change the input layer or adjust the image dimensions, you should consider the following factors:\n",
        "\n",
        "### 1. **Recommendation for Gaming: Using the Pre-trained Model as Is (Adjusting the Image Dimensions)**:\n",
        "   - **Pre-trained Models**: The MobileNetV2 model is pre-trained on the ImageNet dataset, where images typically have dimensions of \\(224 \\times 224 \\times 3\\), but in your code, you're using \\(128 \\times 128 \\times 3\\). This means the model expects square images with specific characteristics.\n",
        "   - **Benefits of Adjusting the Image**:\n",
        "     - **Preserving Pre-trained Weights**: By resizing your \\(32 \\times 128 \\times 3\\) images to \\(128 \\times 128 \\times 3\\), you maintain the integrity of the pre-trained weights. The model is designed to work with square images, and resizing non-square images to fit this shape is a common practice.\n",
        "     - **Consistency with Pre-training**: The model's filters and features are designed around the input shape it was trained on. Resizing to square shapes allows you to take full advantage of these pre-trained features.\n",
        "\n",
        "   - **How to Adjust the Image**:\n",
        "     ```python\n",
        "     image = cv2.resize(image, (128, 128))  # Resize to the required input shape\n",
        "     ```\n",
        "\n",
        "   - **Potential Downsides**:\n",
        "     - **Distortion**: Resizing non-square images to square dimensions can introduce distortion, which might affect the model's performance if the aspect ratio is important for your task. However, this can often be mitigated by using appropriate data augmentation techniques.\n",
        "\n",
        "     **See code below for solution to distortion by adding padding to existing image to square it.**\n",
        "\n",
        "### 2. **Changing the Input Layer (Keeping the Image Dimensions as Is)**:\n",
        "   - **Custom Input Shape**: Alternatively, you could modify the input shape of the model to accept \\(32 \\times 128 \\times 3\\) images. This would involve adjusting the `input_shape` parameter when loading the MobileNetV2 base model.\n",
        "   - **Implications of Changing Input Shape**:\n",
        "     - **Re-training or Fine-tuning**: By changing the input shape to a non-square shape like \\(32 \\times 128 \\times 3\\), you are altering the architecture that the pre-trained weights were designed for. This means that the pre-trained weights may not work as effectively, and you may need to fine-tune the model on your specific dataset.\n",
        "     - **Loss of Pre-trained Model Benefits**: The pre-trained filters in the early layers are designed to work with square images, so changing the input shape could reduce the effectiveness of these pre-trained features.\n",
        "\n",
        "   - **How to Change the Input Layer**: (Not recommended)\n",
        "     ```python\n",
        "     base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 128, 3))\n",
        "     ```\n",
        "\n",
        "   - **Potential Benefits**:\n",
        "     - **No Image Distortion**: You avoid distorting the image by keeping its original aspect ratio.\n",
        "     - **Direct Processing**: The model processes the images in their natural form, which might be beneficial if the aspect ratio is critical for your task.\n",
        "\n",
        "   - **Potential Downsides**:\n",
        "     - **Need for Fine-tuning**: You might need to fine-tune the entire model on your dataset to adjust for the new input shape.\n",
        "\n",
        "### Recommendation:\n",
        "\n",
        "**Adjusting the Image** to fit the input shape expected by the pre-trained model is generally the better approach, especially if:\n",
        "- You want to leverage the pre-trained weights effectively.\n",
        "- You have limited data for fine-tuning and don't want to lose the benefits of pre-training.\n",
        "\n",
        "However, **Changing the Input Layer** might be more appropriate if:\n",
        "- The aspect ratio of your images is critical, and you want to preserve it without distortion.\n",
        "- You are prepared to fine-tune or even re-train the model to account for the new input shape.\n",
        "\n",
        "If you choose to resize your images, consider using data augmentation techniques (like padding or cropping) to help mitigate any potential distortion effects. If you decide to change the input layer, be prepared for a more extensive fine-tuning process."
      ],
      "metadata": {
        "id": "Nh35ZKgC1Lcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 1. Predict Gesture from Path"
      ],
      "metadata": {
        "id": "EtRIfrlGyEDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries and imports needed\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "0: \"Up\",\n",
        "1: \"Down\",\n",
        "2: \"Left\",\n",
        "3: \"Right\",\n",
        "4: \"Straight\" # Add more labels as needed\n",
        "}\n",
        "\n",
        "# Load your pre-trained model\n",
        "\n",
        "model_dir = ''\n",
        "model_name = 'mobilenetv2_head_gesture_model.keras'\n",
        "model = load_model(f'{model_dir}{model_name}')\n",
        "\n",
        "# Function to pad image to make it square and not distort on resize\n",
        "def pad_to_square(image, desired_size=128):\n",
        "    old_size = image.shape[:2]  # (height, width)\n",
        "    ratio = float(desired_size) / max(old_size)\n",
        "\n",
        "    # Compute new size to maintain aspect ratio\n",
        "    new_size = tuple([int(x * ratio) for x in old_size])\n",
        "\n",
        "    # Resize image\n",
        "    image_resized = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "\n",
        "    # Compute padding\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "\n",
        "    # Add padding\n",
        "    color = [0, 0, 0]  # Black padding\n",
        "    new_image = cv2.copyMakeBorder(image_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "# Function to preprocess and predict gesture from path\n",
        "def predict_gesture_pth(image_path):\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Pad image to make it square\n",
        "    padded_image = pad_to_square(image, desired_size=128)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    padded_image = padded_image.astype('float32') / 255.0\n",
        "\n",
        "    # Add batch dimension\n",
        "    padded_image = np.expand_dims(padded_image, axis=0)\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(padded_image)\n",
        "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# Example usage\n",
        "image_path = 'example_image.png'  # Replace with the actual path to your image\n",
        "predicted_class = predict_gesture_pth(image_path)\n",
        "\n",
        "# Print the predicted class\n",
        "print(f'Predicted class: {predicted_class}')"
      ],
      "metadata": {
        "id": "AkFHb0JqxCbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: Predict Gesture from Image"
      ],
      "metadata": {
        "id": "2A5kF6JO5OqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your pre-trained model\n",
        "model_dir = ''\n",
        "model_name = 'mobilenetv2_head_gesture_model.keras'\n",
        "model = load_model(f'{model_dir}{model_name}')\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {\n",
        "    0: \"Up\",\n",
        "    1: \"Down\",\n",
        "    2: \"Left\",\n",
        "    3: \"Right\",\n",
        "    4: \"Straight\"  # Add more labels as needed\n",
        "}\n",
        "\n",
        "# Function to pad image to make it square\n",
        "def pad_to_square(image, desired_size=128):\n",
        "    old_size = image.shape[:2]  # (height, width)\n",
        "    ratio = float(desired_size) / max(old_size)\n",
        "\n",
        "    # Compute new size to maintain aspect ratio\n",
        "    new_size = tuple([int(x * ratio) for x in old_size])\n",
        "\n",
        "    # Resize image\n",
        "    image_resized = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "\n",
        "    # Compute padding\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "\n",
        "    # Add padding\n",
        "    color = [0, 0, 0]  # Black padding\n",
        "    new_image = cv2.copyMakeBorder(image_resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "# Function to preprocess and predict gesture from image\n",
        "def predict_gesture_img(image):\n",
        "    # Pad image to make it square\n",
        "    padded_image = pad_to_square(image, desired_size=128)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    padded_image = padded_image.astype('float32') / 255.0\n",
        "\n",
        "    # Add batch dimension\n",
        "    padded_image = np.expand_dims(padded_image, axis=0)\n",
        "\n",
        "    # Predict using the model\n",
        "    predictions = model.predict(padded_image)\n",
        "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
        "\n",
        "    return predicted_class\n",
        "\n",
        "# Initialize camera\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Flip the frame horizontally\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Predict gesture\n",
        "    predicted_class = predict_gesture_img(frame)\n",
        "    gesture_label = class_labels[predicted_class]\n",
        "\n",
        "    # Display the resulting frame with prediction\n",
        "    cv2.putText(frame, f\"Gesture: {gesture_label}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2.imshow('Head Gesture Recognition', frame)\n",
        "\n",
        "    # Exit on 'q' key press\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# When everything done, release the capture\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "yP994cqnxCWu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}